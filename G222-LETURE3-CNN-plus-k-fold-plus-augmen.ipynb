{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import *\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARwAAAD8CAYAAAClxxvWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d33840860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the former 6 train images below\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(321)\n",
    "plt.imshow(x_train[0],cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(322)\n",
    "plt.imshow(x_train[1],cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(323)\n",
    "plt.imshow(x_train[2],cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(324)\n",
    "plt.imshow(x_train[3],cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(325)\n",
    "plt.imshow(x_train[4],cmap=plt.get_cmap('gray'))\n",
    "\n",
    "plt.subplot(326)\n",
    "plt.imshow(x_train[5],cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':  ##reshape it so that it is suitable for use training a CNN. \n",
    "    #In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height].\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols).astype('float32')\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In order to speed up the convergence, we may normalize the input values\n",
    "# so that they are in the range of (0, 1) for (-1, 1)\n",
    "# Your code here.\n",
    "x_train =x_train/255.0\n",
    "x_test =x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please build up a simple ConvNets by stacking a few conovlutioanl layers (kenel size with 3x3\n",
    "# is a good choice, don't foget using non-linear activations for convolutional layers),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extra Points 2: use K-Fold cross-validation for ensembling k models,\n",
    "# i.e. (1) split the whole training data into K folds;\n",
    "#      (2) train K models based on different training data;\n",
    "#      (3) when evaludating the testing data, averaging over K model predictions as final output.\n",
    "# The codes may look like:\n",
    "#   for i in range(K):\n",
    "#       x_train, y_train = ...\n",
    "#       model_i = train(x_train , y_train)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#split traning set into k fold\n",
    "n_folds = 10\n",
    "ss = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "468/468 [==============================] - 20s - loss: 0.5872 - acc: 0.8091 - val_loss: 0.0732 - val_acc: 0.9787\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 17s - loss: 0.1769 - acc: 0.9452 - val_loss: 0.0491 - val_acc: 0.9842\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 17s - loss: 0.1277 - acc: 0.9607 - val_loss: 0.0372 - val_acc: 0.9867\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 17s - loss: 0.1040 - acc: 0.9667 - val_loss: 0.0332 - val_acc: 0.9895\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 17s - loss: 0.0928 - acc: 0.9710 - val_loss: 0.0292 - val_acc: 0.9905\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 17s - loss: 0.0818 - acc: 0.9749 - val_loss: 0.0289 - val_acc: 0.9905\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 17s - loss: 0.0728 - acc: 0.9773 - val_loss: 0.0246 - val_acc: 0.9918\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 17s - loss: 0.0701 - acc: 0.9784 - val_loss: 0.0247 - val_acc: 0.9920\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 17s - loss: 0.0666 - acc: 0.9794 - val_loss: 0.0257 - val_acc: 0.9920\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 17s - loss: 0.0623 - acc: 0.9806 - val_loss: 0.0249 - val_acc: 0.9927\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 17s - loss: 0.0581 - acc: 0.9823 - val_loss: 0.0263 - val_acc: 0.9907\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 17s - loss: 0.0536 - acc: 0.9829 - val_loss: 0.0238 - val_acc: 0.9923\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 17s - loss: 0.0519 - acc: 0.9835 - val_loss: 0.0233 - val_acc: 0.9915\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 17s - loss: 0.0536 - acc: 0.9838 - val_loss: 0.0218 - val_acc: 0.9923\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 17s - loss: 0.0496 - acc: 0.9847 - val_loss: 0.0226 - val_acc: 0.9922\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 17s - loss: 0.0472 - acc: 0.9848 - val_loss: 0.0239 - val_acc: 0.9908\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 17s - loss: 0.0468 - acc: 0.9854 - val_loss: 0.0206 - val_acc: 0.9922\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 17s - loss: 0.0459 - acc: 0.9857 - val_loss: 0.0192 - val_acc: 0.9938\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 17s - loss: 0.0431 - acc: 0.9865 - val_loss: 0.0179 - val_acc: 0.9935\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 17s - loss: 0.0424 - acc: 0.9866 - val_loss: 0.0168 - val_acc: 0.9947\n",
      "Training complete!\n",
      "Number of models: 1\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 18s - loss: 0.5918 - acc: 0.8068 - val_loss: 0.0763 - val_acc: 0.9767\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 17s - loss: 0.1875 - acc: 0.9415 - val_loss: 0.0518 - val_acc: 0.9812\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 18s - loss: 0.1391 - acc: 0.9566 - val_loss: 0.0421 - val_acc: 0.9853\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 17s - loss: 0.1103 - acc: 0.9664 - val_loss: 0.0360 - val_acc: 0.9880\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 17s - loss: 0.0986 - acc: 0.9692 - val_loss: 0.0326 - val_acc: 0.9888\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 17s - loss: 0.0889 - acc: 0.9726 - val_loss: 0.0303 - val_acc: 0.9905\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 17s - loss: 0.0807 - acc: 0.9756 - val_loss: 0.0284 - val_acc: 0.9897\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 17s - loss: 0.0751 - acc: 0.9773 - val_loss: 0.0256 - val_acc: 0.9910\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 17s - loss: 0.0696 - acc: 0.9783 - val_loss: 0.0238 - val_acc: 0.9913\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 17s - loss: 0.0651 - acc: 0.9800 - val_loss: 0.0216 - val_acc: 0.9927\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 18s - loss: 0.0444 - acc: 0.9861 - val_loss: 0.0207 - val_acc: 0.9927\n",
      "Training complete!\n",
      "Number of models: 2\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 18s - loss: 0.5936 - acc: 0.8050 - val_loss: 0.0967 - val_acc: 0.9682\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 18s - loss: 0.1765 - acc: 0.9454 - val_loss: 0.0594 - val_acc: 0.9842\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 18s - loss: 0.1289 - acc: 0.9597 - val_loss: 0.0546 - val_acc: 0.9837\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 17s - loss: 0.1089 - acc: 0.9660 - val_loss: 0.0415 - val_acc: 0.9888\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 17s - loss: 0.0919 - acc: 0.9715 - val_loss: 0.0400 - val_acc: 0.9890\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 17s - loss: 0.0836 - acc: 0.9726 - val_loss: 0.0396 - val_acc: 0.9893\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 17s - loss: 0.0748 - acc: 0.9768 - val_loss: 0.0348 - val_acc: 0.9895\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 17s - loss: 0.0702 - acc: 0.9782 - val_loss: 0.0322 - val_acc: 0.9905\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 17s - loss: 0.0652 - acc: 0.9791 - val_loss: 0.0308 - val_acc: 0.9920\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 17s - loss: 0.0610 - acc: 0.9811 - val_loss: 0.0303 - val_acc: 0.9918\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 17s - loss: 0.0588 - acc: 0.9828 - val_loss: 0.0295 - val_acc: 0.9930\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 17s - loss: 0.0546 - acc: 0.9829 - val_loss: 0.0299 - val_acc: 0.9915\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 18s - loss: 0.0552 - acc: 0.9830 - val_loss: 0.0297 - val_acc: 0.9922\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 17s - loss: 0.0517 - acc: 0.9833 - val_loss: 0.0258 - val_acc: 0.9927\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 17s - loss: 0.0507 - acc: 0.9841 - val_loss: 0.0312 - val_acc: 0.9903\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 18s - loss: 0.0484 - acc: 0.9850 - val_loss: 0.0254 - val_acc: 0.9932\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 17s - loss: 0.0458 - acc: 0.9857 - val_loss: 0.0233 - val_acc: 0.9948\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 17s - loss: 0.0445 - acc: 0.9857 - val_loss: 0.0251 - val_acc: 0.9927\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 17s - loss: 0.0438 - acc: 0.9864 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 17s - loss: 0.0426 - acc: 0.9869 - val_loss: 0.0255 - val_acc: 0.9937\n",
      "Training complete!\n",
      "Number of models: 3\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 18s - loss: 0.5824 - acc: 0.8110 - val_loss: 0.0789 - val_acc: 0.9768\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 18s - loss: 0.1835 - acc: 0.9428 - val_loss: 0.0532 - val_acc: 0.9832\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 18s - loss: 0.1346 - acc: 0.9580 - val_loss: 0.0435 - val_acc: 0.9862\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 18s - loss: 0.1086 - acc: 0.9656 - val_loss: 0.0362 - val_acc: 0.9877\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 18s - loss: 0.0902 - acc: 0.9717 - val_loss: 0.0351 - val_acc: 0.9887\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 18s - loss: 0.0861 - acc: 0.9730 - val_loss: 0.0425 - val_acc: 0.9878\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 18s - loss: 0.0767 - acc: 0.9762 - val_loss: 0.0315 - val_acc: 0.9903\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 18s - loss: 0.0692 - acc: 0.9778 - val_loss: 0.0277 - val_acc: 0.9915\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 18s - loss: 0.0664 - acc: 0.9795 - val_loss: 0.0278 - val_acc: 0.9922\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 18s - loss: 0.0604 - acc: 0.9812 - val_loss: 0.0274 - val_acc: 0.9907\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 18s - loss: 0.0575 - acc: 0.9825 - val_loss: 0.0245 - val_acc: 0.9915\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 18s - loss: 0.0574 - acc: 0.9825 - val_loss: 0.0263 - val_acc: 0.9915\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 18s - loss: 0.0528 - acc: 0.9836 - val_loss: 0.0245 - val_acc: 0.9923\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 18s - loss: 0.0501 - acc: 0.9841 - val_loss: 0.0268 - val_acc: 0.9915\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 18s - loss: 0.0500 - acc: 0.9843 - val_loss: 0.0256 - val_acc: 0.9927\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 18s - loss: 0.0480 - acc: 0.9853 - val_loss: 0.0204 - val_acc: 0.9940\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 18s - loss: 0.0467 - acc: 0.9856 - val_loss: 0.0232 - val_acc: 0.9923\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 18s - loss: 0.0469 - acc: 0.9853 - val_loss: 0.0230 - val_acc: 0.9923\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 18s - loss: 0.0439 - acc: 0.9865 - val_loss: 0.0208 - val_acc: 0.9928\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 18s - loss: 0.0421 - acc: 0.9871 - val_loss: 0.0209 - val_acc: 0.9937\n",
      "Training complete!\n",
      "Number of models: 4\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 19s - loss: 0.0459 - acc: 0.9857 - val_loss: 0.0219 - val_acc: 0.9942\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 19s - loss: 0.0455 - acc: 0.9860 - val_loss: 0.0237 - val_acc: 0.9943\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 19s - loss: 0.0441 - acc: 0.9865 - val_loss: 0.0206 - val_acc: 0.9943\n",
      "Training complete!\n",
      "Number of models: 5\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 19s - loss: 0.5762 - acc: 0.8106 - val_loss: 0.0830 - val_acc: 0.9747\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 19s - loss: 0.1715 - acc: 0.9472 - val_loss: 0.0514 - val_acc: 0.9835\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 19s - loss: 0.1282 - acc: 0.9601 - val_loss: 0.0439 - val_acc: 0.9847\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 19s - loss: 0.1081 - acc: 0.9671 - val_loss: 0.0378 - val_acc: 0.9880\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 19s - loss: 0.0935 - acc: 0.9708 - val_loss: 0.0286 - val_acc: 0.9905\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 19s - loss: 0.0818 - acc: 0.9744 - val_loss: 0.0332 - val_acc: 0.9900\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 19s - loss: 0.0734 - acc: 0.9775 - val_loss: 0.0279 - val_acc: 0.9912\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 19s - loss: 0.0700 - acc: 0.9786 - val_loss: 0.0262 - val_acc: 0.9910\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 19s - loss: 0.0659 - acc: 0.9794 - val_loss: 0.0222 - val_acc: 0.9922\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 19s - loss: 0.0620 - acc: 0.9815 - val_loss: 0.0220 - val_acc: 0.9927\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 19s - loss: 0.0604 - acc: 0.9812 - val_loss: 0.0211 - val_acc: 0.9923\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 19s - loss: 0.0567 - acc: 0.9820 - val_loss: 0.0207 - val_acc: 0.9927\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 19s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.0225 - val_acc: 0.9923\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 19s - loss: 0.0540 - acc: 0.9836 - val_loss: 0.0236 - val_acc: 0.9917\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 19s - loss: 0.0499 - acc: 0.9844 - val_loss: 0.0185 - val_acc: 0.9930\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 19s - loss: 0.0500 - acc: 0.9848 - val_loss: 0.0187 - val_acc: 0.9938\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 19s - loss: 0.0470 - acc: 0.9852 - val_loss: 0.0202 - val_acc: 0.9930\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 19s - loss: 0.0488 - acc: 0.9849 - val_loss: 0.0195 - val_acc: 0.9930\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 19s - loss: 0.0454 - acc: 0.9857 - val_loss: 0.0189 - val_acc: 0.9937\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 19s - loss: 0.0435 - acc: 0.9861 - val_loss: 0.0159 - val_acc: 0.9940\n",
      "Training complete!\n",
      "Number of models: 6\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 19s - loss: 0.5773 - acc: 0.8101 - val_loss: 0.0846 - val_acc: 0.9745\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 19s - loss: 0.1729 - acc: 0.9464 - val_loss: 0.0536 - val_acc: 0.9847\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 19s - loss: 0.1270 - acc: 0.9598 - val_loss: 0.0484 - val_acc: 0.9850\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 19s - loss: 0.1086 - acc: 0.9663 - val_loss: 0.0418 - val_acc: 0.9882\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 19s - loss: 0.0931 - acc: 0.9709 - val_loss: 0.0377 - val_acc: 0.9890\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 19s - loss: 0.0829 - acc: 0.9737 - val_loss: 0.0345 - val_acc: 0.9903\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 19s - loss: 0.0748 - acc: 0.9768 - val_loss: 0.0335 - val_acc: 0.9898\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 19s - loss: 0.0719 - acc: 0.9782 - val_loss: 0.0345 - val_acc: 0.9898\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 19s - loss: 0.0652 - acc: 0.9794 - val_loss: 0.0308 - val_acc: 0.9910\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 19s - loss: 0.0637 - acc: 0.9803 - val_loss: 0.0288 - val_acc: 0.9918\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 19s - loss: 0.0600 - acc: 0.9811 - val_loss: 0.0263 - val_acc: 0.9933\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 19s - loss: 0.0557 - acc: 0.9827 - val_loss: 0.0253 - val_acc: 0.9928\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 19s - loss: 0.0543 - acc: 0.9832 - val_loss: 0.0294 - val_acc: 0.9917\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 19s - loss: 0.0521 - acc: 0.9837 - val_loss: 0.0280 - val_acc: 0.9922\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 19s - loss: 0.0494 - acc: 0.9845 - val_loss: 0.0258 - val_acc: 0.9918\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 19s - loss: 0.0484 - acc: 0.9850 - val_loss: 0.0259 - val_acc: 0.9927\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 19s - loss: 0.0455 - acc: 0.9854 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 19s - loss: 0.0455 - acc: 0.9861 - val_loss: 0.0261 - val_acc: 0.9927\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 19s - loss: 0.0438 - acc: 0.9866 - val_loss: 0.0253 - val_acc: 0.9930\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 19s - loss: 0.0439 - acc: 0.9866 - val_loss: 0.0236 - val_acc: 0.9933\n",
      "Training complete!\n",
      "Number of models: 7\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 19s - loss: 0.5958 - acc: 0.8058 - val_loss: 0.0772 - val_acc: 0.9765\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 19s - loss: 0.1761 - acc: 0.9456 - val_loss: 0.0615 - val_acc: 0.9822\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 19s - loss: 0.1329 - acc: 0.9589 - val_loss: 0.0511 - val_acc: 0.9853\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 19s - loss: 0.1118 - acc: 0.9648 - val_loss: 0.0435 - val_acc: 0.9868\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 19s - loss: 0.0929 - acc: 0.9710 - val_loss: 0.0366 - val_acc: 0.9903\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 19s - loss: 0.0848 - acc: 0.9728 - val_loss: 0.0387 - val_acc: 0.9888\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 19s - loss: 0.0797 - acc: 0.9752 - val_loss: 0.0390 - val_acc: 0.9885\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 19s - loss: 0.0719 - acc: 0.9777 - val_loss: 0.0348 - val_acc: 0.9907\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 19s - loss: 0.0675 - acc: 0.9788 - val_loss: 0.0307 - val_acc: 0.9918\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 19s - loss: 0.0647 - acc: 0.9797 - val_loss: 0.0346 - val_acc: 0.9905\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 19s - loss: 0.0579 - acc: 0.9822 - val_loss: 0.0334 - val_acc: 0.9908\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 19s - loss: 0.0584 - acc: 0.9824 - val_loss: 0.0311 - val_acc: 0.9912\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 19s - loss: 0.0549 - acc: 0.9829 - val_loss: 0.0294 - val_acc: 0.9922\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 19s - loss: 0.0550 - acc: 0.9833 - val_loss: 0.0283 - val_acc: 0.9918\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 19s - loss: 0.0497 - acc: 0.9846 - val_loss: 0.0327 - val_acc: 0.9910\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 19s - loss: 0.0527 - acc: 0.9840 - val_loss: 0.0269 - val_acc: 0.9922\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 19s - loss: 0.0477 - acc: 0.9853 - val_loss: 0.0280 - val_acc: 0.9932\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 19s - loss: 0.0473 - acc: 0.9847 - val_loss: 0.0287 - val_acc: 0.9925\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 19s - loss: 0.0461 - acc: 0.9863 - val_loss: 0.0249 - val_acc: 0.9937\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 19s - loss: 0.0452 - acc: 0.9858 - val_loss: 0.0277 - val_acc: 0.9920\n",
      "Training complete!\n",
      "Number of models: 8\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 20s - loss: 0.5887 - acc: 0.8077 - val_loss: 0.0791 - val_acc: 0.9728\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 19s - loss: 0.1718 - acc: 0.9463 - val_loss: 0.0531 - val_acc: 0.9825\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 19s - loss: 0.1312 - acc: 0.9591 - val_loss: 0.0433 - val_acc: 0.9868\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 19s - loss: 0.1071 - acc: 0.9668 - val_loss: 0.0412 - val_acc: 0.9867\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 19s - loss: 0.0945 - acc: 0.9714 - val_loss: 0.0367 - val_acc: 0.9892\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 19s - loss: 0.0885 - acc: 0.9728 - val_loss: 0.0352 - val_acc: 0.9880\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 19s - loss: 0.0763 - acc: 0.9765 - val_loss: 0.0359 - val_acc: 0.9887\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 19s - loss: 0.0696 - acc: 0.9790 - val_loss: 0.0331 - val_acc: 0.9907\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 19s - loss: 0.0656 - acc: 0.9799 - val_loss: 0.0312 - val_acc: 0.9902\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 19s - loss: 0.0624 - acc: 0.9804 - val_loss: 0.0278 - val_acc: 0.9920\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 19s - loss: 0.0584 - acc: 0.9821 - val_loss: 0.0274 - val_acc: 0.9918\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 19s - loss: 0.0569 - acc: 0.9824 - val_loss: 0.0274 - val_acc: 0.9928\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 19s - loss: 0.0522 - acc: 0.9834 - val_loss: 0.0253 - val_acc: 0.9935\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 19s - loss: 0.0530 - acc: 0.9835 - val_loss: 0.0275 - val_acc: 0.9922\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 19s - loss: 0.0492 - acc: 0.9846 - val_loss: 0.0297 - val_acc: 0.9920\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 19s - loss: 0.0483 - acc: 0.9849 - val_loss: 0.0248 - val_acc: 0.9937\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 19s - loss: 0.0480 - acc: 0.9849 - val_loss: 0.0273 - val_acc: 0.9942\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 19s - loss: 0.0448 - acc: 0.9854 - val_loss: 0.0245 - val_acc: 0.9942\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 19s - loss: 0.0442 - acc: 0.9860 - val_loss: 0.0263 - val_acc: 0.9935\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 19s - loss: 0.0427 - acc: 0.9873 - val_loss: 0.0261 - val_acc: 0.9943\n",
      "Training complete!\n",
      "Number of models: 9\n",
      "Epoch 1/20\n",
      "468/468 [==============================] - 20s - loss: 0.5766 - acc: 0.8110 - val_loss: 0.0784 - val_acc: 0.9768\n",
      "Epoch 2/20\n",
      "468/468 [==============================] - 20s - loss: 0.1718 - acc: 0.9471 - val_loss: 0.0480 - val_acc: 0.9843\n",
      "Epoch 3/20\n",
      "468/468 [==============================] - 20s - loss: 0.1269 - acc: 0.9597 - val_loss: 0.0390 - val_acc: 0.9877\n",
      "Epoch 4/20\n",
      "468/468 [==============================] - 20s - loss: 0.1053 - acc: 0.9683 - val_loss: 0.0437 - val_acc: 0.9858\n",
      "Epoch 5/20\n",
      "468/468 [==============================] - 20s - loss: 0.0899 - acc: 0.9716 - val_loss: 0.0288 - val_acc: 0.9903\n",
      "Epoch 6/20\n",
      "468/468 [==============================] - 20s - loss: 0.0846 - acc: 0.9744 - val_loss: 0.0314 - val_acc: 0.9905\n",
      "Epoch 7/20\n",
      "468/468 [==============================] - 20s - loss: 0.0744 - acc: 0.9768 - val_loss: 0.0283 - val_acc: 0.9898\n",
      "Epoch 8/20\n",
      "468/468 [==============================] - 20s - loss: 0.0690 - acc: 0.9788 - val_loss: 0.0270 - val_acc: 0.9918\n",
      "Epoch 9/20\n",
      "468/468 [==============================] - 20s - loss: 0.0687 - acc: 0.9791 - val_loss: 0.0252 - val_acc: 0.9923\n",
      "Epoch 10/20\n",
      "468/468 [==============================] - 20s - loss: 0.0641 - acc: 0.9807 - val_loss: 0.0271 - val_acc: 0.9915\n",
      "Epoch 11/20\n",
      "468/468 [==============================] - 19s - loss: 0.0596 - acc: 0.9818 - val_loss: 0.0235 - val_acc: 0.9922\n",
      "Epoch 12/20\n",
      "468/468 [==============================] - 19s - loss: 0.0563 - acc: 0.9826 - val_loss: 0.0227 - val_acc: 0.9927\n",
      "Epoch 13/20\n",
      "468/468 [==============================] - 19s - loss: 0.0550 - acc: 0.9829 - val_loss: 0.0187 - val_acc: 0.9943\n",
      "Epoch 14/20\n",
      "468/468 [==============================] - 19s - loss: 0.0514 - acc: 0.9835 - val_loss: 0.0192 - val_acc: 0.9932\n",
      "Epoch 15/20\n",
      "468/468 [==============================] - 19s - loss: 0.0520 - acc: 0.9831 - val_loss: 0.0193 - val_acc: 0.9935\n",
      "Epoch 16/20\n",
      "468/468 [==============================] - 20s - loss: 0.0478 - acc: 0.9860 - val_loss: 0.0165 - val_acc: 0.9950\n",
      "Epoch 17/20\n",
      "468/468 [==============================] - 20s - loss: 0.0462 - acc: 0.9850 - val_loss: 0.0184 - val_acc: 0.9942\n",
      "Epoch 18/20\n",
      "468/468 [==============================] - 20s - loss: 0.0455 - acc: 0.9859 - val_loss: 0.0191 - val_acc: 0.9940\n",
      "Epoch 19/20\n",
      "468/468 [==============================] - 20s - loss: 0.0458 - acc: 0.9856 - val_loss: 0.0190 - val_acc: 0.9940\n",
      "Epoch 20/20\n",
      "468/468 [==============================] - 20s - loss: 0.0423 - acc: 0.9868 - val_loss: 0.0190 - val_acc: 0.9943\n",
      "Training complete!\n",
      "Number of models: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99609999999999999"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#judge if there is an augmentation\n",
    "# Please build up a simple ConvNets by stacking a few conovlutioanl layers (kenel size with 3x3\n",
    "# is a good choice, don't foget using non-linear activations for convolutional layers),\n",
    "# max-pooling layers, dropout layers and dense/fully-connected layers.\n",
    "# Set the CNN model \n",
    "# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Out\n",
    "\n",
    "def train_model(train_x, train_y, test_x, test_y,augmentation=True): \n",
    "    model = Sequential()  # create a Sequential model by passing a list of layer instances to the constructor\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',input_shape=input_shape))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "    model.add(Dropout(0.2))##drop 20% data\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adadelta(),  ##use ADAM here\n",
    "              metrics=['accuracy'])\n",
    "     \n",
    "    if augmentation is True:\n",
    "        datagen=ImageDataGenerator(\n",
    "              zoom_range=0.1,\n",
    "              width_shift_range=0.1,\n",
    "              height_shift_range=0.1,\n",
    "              rotation_range=30)\n",
    "        datagen.fit(x_train)\n",
    "        \n",
    "        model.fit_generator(datagen.flow(train_x, train_y, batch_size=batch_size),\n",
    "        steps_per_epoch= int(len(x_train) / batch_size),\n",
    "        epochs=epochs, \n",
    "        verbose=1,\n",
    "        validation_data=(test_x, test_y))\n",
    "        print(\"Training complete!\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        model.fit(train_x, train_y, batch_size=batch_size,\n",
    "                 epochs=epochs, verbose=0, validation_data=(test_x, test_y))\n",
    "    \n",
    "        print(\"Training complete\")\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "ensemble_models = []\n",
    "\n",
    "for train, test in ss.split(x_train, np.argmax(y_train, 1)):  #print the i th ensemble models are training \n",
    "    ensemble_models.append(train_model(x_train[train], y_train[train], x_train[test], y_train[test]))\n",
    "    print('Number of models: {}'.format(len(ensemble_models)))\n",
    "\n",
    "    \n",
    "def predict(x_test):\n",
    "    predictions = np.array([model.predict(x_test) for model in ensemble_models])\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "def accuracy(prediction, y_test):\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    labels = np.argmax(y_test, axis=1)\n",
    "    return np.mean(prediction == labels)\n",
    "\n",
    "prediction = predict(x_test)\n",
    "accuracy(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
